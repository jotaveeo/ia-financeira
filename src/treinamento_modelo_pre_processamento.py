import pandas as pd
import numpy as np
import re
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from imblearn.over_sampling import SMOTE
from scipy.sparse import hstack
import warnings
from processador_texto import ProcessadorTexto

warnings.filterwarnings('ignore')

print("üö® SCRIPT DE TREINAMENTO CORRIGIDO - SOLU√á√ÉO EMERGENCIAL")
print("Configurado para usar o dataset emergencial que resolve problemas de produ√ß√£o")
print("=" * 70)

# Configura√ß√µes
RANDOM_STATE = 42
TEST_SIZE = 0.2
CV_FOLDS = 3
MAX_FEATURES = 2000
MIN_DF = 2
MAX_DF = 0.95
SMOTE_K_NEIGHBORS = 5

def carregar_dados():
    """Carrega dados de treinamento - CORRIGIDO PARA USAR DATASET EMERGENCIAL"""
    try:
        # SOLU√á√ÉO EMERGENCIAL: Usar dataset corrigido primeiro
        df = pd.read_csv('transacoes_emergencial_producao.csv')
        print("‚úÖ Usando transacoes_emergencial_producao.csv (DATASET EMERGENCIAL)")
        print("üéØ Este dataset foi criado especificamente para resolver os problemas de produ√ß√£o")
    except FileNotFoundError:
        try:
            df = pd.read_csv('transacoes_melhorado.csv')
            print("‚ö†Ô∏è  Usando transacoes_melhorado.csv (dataset antigo)")
            print("üö® ATEN√á√ÉO: Para melhor performance, use o dataset emergencial!")
        except FileNotFoundError:
            try:
                df = pd.read_csv('transacoes_exemplo.csv')
                print("‚ö†Ô∏è  Usando transacoes_exemplo.csv (dataset b√°sico)")
            except FileNotFoundError:
                print("‚ùå Erro: Nenhum arquivo de dados encontrado!")
                return None
    
    print(f"üìä Dataset: {len(df)} transa√ß√µes, {df['Categoria'].nunique()} categorias")
    
    # Verificar se tem as categorias cr√≠ticas
    categorias_criticas = ["Transporte", "Streaming", "Alimenta√ß√£o", "Supermercado"]
    print(f"\nüéØ VERIFICA√á√ÉO DAS CATEGORIAS CR√çTICAS:")
    for cat in categorias_criticas:
        count = len(df[df['Categoria'] == cat]) if cat in df['Categoria'].values else 0
        status = "‚úÖ" if count >= 30 else "‚ö†Ô∏è"
        print(f"  {status} {cat}: {count} exemplos")
    
    # Mostrar distribui√ß√£o
    print(f"\nüìà Top 10 categorias:")
    print(df['Categoria'].value_counts().head(10))
    
    # Verificar se tem "Taxas Banc√°rias" (categoria problem√°tica)
    if "Taxas Banc√°rias" in df['Categoria'].values:
        count_taxas = len(df[df['Categoria'] == 'Taxas Banc√°rias'])
        print(f"\nüö® ATEN√á√ÉO: Dataset cont√©m {count_taxas} exemplos de 'Taxas Banc√°rias'")
        print(f"   Esta categoria causa 40% dos erros em produ√ß√£o!")
        print(f"   Recomenda√ß√£o: Use o dataset emergencial que remove esta categoria")
    else:
        print(f"\n‚úÖ √ìTIMO: Dataset n√£o cont√©m 'Taxas Banc√°rias' (categoria problem√°tica removida)")
    
    return df

class TreinadorModeloCorrigido:
    """Classe para treinamento do modelo - VERS√ÉO CORRIGIDA"""
    
    def __init__(self):
        self.processador_texto = ProcessadorTexto()
        self.vectorizer = None
        self.scaler = None
        self.modelo_final = None
        self.melhor_algoritmo = None
        self.categorias_problematicas = ["Taxas Banc√°rias"]  # Lista de categorias a evitar
        
    def preprocessar_dados(self, df):
        """Pr√©-processamento dos dados - VERS√ÉO MELHORADA"""
        print("üîß Pr√©-processando dados...")
        
        # Verificar e remover categorias problem√°ticas se existirem
        categorias_antes = df['Categoria'].nunique()
        for cat_prob in self.categorias_problematicas:
            if cat_prob in df['Categoria'].values:
                count_removidas = len(df[df['Categoria'] == cat_prob])
                df = df[df['Categoria'] != cat_prob]
                print(f"üö® Removidas {count_removidas} transa√ß√µes da categoria problem√°tica: '{cat_prob}'")
        
        categorias_depois = df['Categoria'].nunique()
        if categorias_antes != categorias_depois:
            print(f"üìä Categorias: {categorias_antes} ‚Üí {categorias_depois} (removidas categorias problem√°ticas)")
        
        # Verificar dados
        print(f"üìä Dados ap√≥s limpeza: {len(df)} transa√ß√µes, {df['Categoria'].nunique()} categorias")
        
        # Processar texto
        print("üî§ Processando descri√ß√µes...")
        df['descricao_processada'] = df['Descri√ß√£o'].apply(self.processador_texto.processar_texto)
        
        # Extrair features num√©ricas
        print("üî¢ Extraindo features num√©ricas...")
        df['valor_abs'] = df['Valor'].abs()
        df['eh_receita'] = (df['Valor'] > 0).astype(int)
        df['valor_log'] = np.log1p(df['valor_abs'])
        
        # Features de texto
        print("üìù Extraindo features de texto...")
        df['tamanho_descricao'] = df['Descri√ß√£o'].str.len()
        df['tem_pix'] = df['Descri√ß√£o'].str.contains('pix|PIX', case=False, na=False).astype(int)
        df['tem_debito'] = df['Descri√ß√£o'].str.contains('d√©bito|debito', case=False, na=False).astype(int)
        df['tem_transferencia'] = df['Descri√ß√£o'].str.contains('transfer√™ncia|transferencia', case=False, na=False).astype(int)
        
        return df
    
    def criar_features(self, df):
        """Cria√ß√£o de features - VERS√ÉO MELHORADA"""
        print("üéØ Criando features...")
        
        # Vectoriza√ß√£o TF-IDF
        print("  üìä Vectoriza√ß√£o TF-IDF...")
        self.vectorizer = TfidfVectorizer(
            max_features=MAX_FEATURES,
            min_df=MIN_DF,
            max_df=MAX_DF,
            ngram_range=(1, 2),
            stop_words=None
        )
        
        X_text = self.vectorizer.fit_transform(df['descricao_processada'])
        
        # Features num√©ricas
        print("  üî¢ Features num√©ricas...")
        features_numericas = ['valor_abs', 'eh_receita', 'valor_log', 'tamanho_descricao', 
                             'tem_pix', 'tem_debito', 'tem_transferencia']
        
        X_num = df[features_numericas].values
        
        # Normalizar features num√©ricas
        self.scaler = StandardScaler()
        X_num_scaled = self.scaler.fit_transform(X_num)
        
        # Combinar features
        print("  üîó Combinando features...")
        X_combined = hstack([X_text, X_num_scaled])
        
        print(f"  ‚úÖ Features criadas: {X_combined.shape[1]} dimens√µes")
        
        return X_combined, df['Categoria']
    
    def treinar_modelos(self, X, y):
        """Treinamento de m√∫ltiplos modelos - VERS√ÉO OTIMIZADA"""
        print("ü§ñ Treinando modelos...")
        
        # Dividir dados
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
        )
        
        print(f"üìä Divis√£o: {X_train.shape[0]} treino, {X_test.shape[0]} teste")
        
        # Aplicar SMOTE para balanceamento
        print("‚öñÔ∏è  Aplicando SMOTE para balanceamento...")
        try:
            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=min(SMOTE_K_NEIGHBORS, len(y_train.unique())-1))
            X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
            print(f"  ‚úÖ Dados balanceados: {X_train_balanced.shape[0]} exemplos")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  SMOTE falhou: {e}")
            X_train_balanced, y_train_balanced = X_train, y_train
        
        # Modelos para testar
        modelos = {
            'Naive Bayes': MultinomialNB(),
            'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),
            'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, n_jobs=-1)
        }
        
        # Par√¢metros para Grid Search
        parametros = {
            'Naive Bayes': {'alpha': [0.1, 0.5, 1.0, 2.0]},
            'Random Forest': {
                'n_estimators': [100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5]
            },
            'Logistic Regression': {
                'C': [0.1, 1.0, 10.0],
                'penalty': ['l2']
            }
        }
        
        resultados = {}
        
        for nome, modelo in modelos.items():
            print(f"\nüîÑ Treinando {nome}...")
            
            try:
                # Grid Search
                grid_search = GridSearchCV(
                    modelo, parametros[nome], 
                    cv=CV_FOLDS, scoring='f1_weighted', 
                    n_jobs=-1, verbose=0
                )
                
                grid_search.fit(X_train_balanced, y_train_balanced)
                
                # Melhor modelo
                melhor_modelo = grid_search.best_estimator_
                
                # Predi√ß√µes
                y_pred = melhor_modelo.predict(X_test)
                
                # M√©tricas
                acuracia = accuracy_score(y_test, y_pred)
                f1 = f1_score(y_test, y_pred, average='weighted')
                
                # Valida√ß√£o cruzada
                cv_scores = cross_val_score(melhor_modelo, X_train_balanced, y_train_balanced, 
                                          cv=CV_FOLDS, scoring='f1_weighted')
                
                resultados[nome] = {
                    'modelo': melhor_modelo,
                    'acuracia': acuracia,
                    'f1_score': f1,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'melhores_params': grid_search.best_params_
                }
                
                print(f"  ‚úÖ {nome}:")
                print(f"     Acur√°cia: {acuracia:.4f}")
                print(f"     F1-Score: {f1:.4f}")
                print(f"     CV: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")
                print(f"     Par√¢metros: {grid_search.best_params_}")
                
            except Exception as e:
                print(f"  ‚ùå Erro no {nome}: {e}")
        
        # Selecionar melhor modelo
        if resultados:
            melhor_nome = max(resultados.keys(), key=lambda k: resultados[k]['f1_score'])
            self.modelo_final = resultados[melhor_nome]['modelo']
            self.melhor_algoritmo = melhor_nome
            
            print(f"\nüèÜ MELHOR MODELO: {melhor_nome}")
            print(f"   Acur√°cia: {resultados[melhor_nome]['acuracia']:.4f}")
            print(f"   F1-Score: {resultados[melhor_nome]['f1_score']:.4f}")
            
            # Relat√≥rio detalhado
            y_pred_final = self.modelo_final.predict(X_test)
            print(f"\nüìä RELAT√ìRIO DETALHADO:")
            print(classification_report(y_test, y_pred_final, zero_division=0))
            
            return X_test, y_test, y_pred_final
        else:
            print("‚ùå Nenhum modelo foi treinado com sucesso!")
            return None, None, None
    
    def salvar_modelo(self):
        """Salva o modelo treinado"""
        print("üíæ Salvando modelo...")
        
        try:
            # Salvar modelo
            joblib.dump(self.modelo_final, 'modelo_final.pkl')
            print("  ‚úÖ modelo_final.pkl")
            
            # Salvar vectorizer
            joblib.dump(self.vectorizer, 'vectorizer_final.pkl')
            print("  ‚úÖ vectorizer_final.pkl")
            
            # Salvar scaler
            joblib.dump(self.scaler, 'scaler_final.pkl')
            print("  ‚úÖ scaler_final.pkl")
            
            # Salvar processador de texto
            joblib.dump(self.processador_texto, 'processador_final.pkl')
            print("  ‚úÖ processador_final.pkl")
            
            print("üíæ Todos os arquivos salvos com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar: {e}")
    
    def demonstracao_pratica(self, df):
        """Demonstra√ß√£o pr√°tica com exemplos reais - VERS√ÉO CORRIGIDA"""
        print("üéØ DEMONSTRA√á√ÉO PR√ÅTICA COM CASOS CR√çTICOS:")
        print("=" * 60)
        
        # Casos cr√≠ticos que falharam em produ√ß√£o
        casos_criticos = [
            "Transfer√™ncia enviada pelo Pix - Uber",
            "Transfer√™ncia enviada pelo Pix - GOGIPSY BRASIL", 
            "Compra no d√©bito - SUPERM SAO LUIZ",
            "Compra no d√©bito - GIL DA TAPIOCA",
            "Aplica√ß√£o RDB",
            "Resgate RDB",
            "Compra no d√©bito - PAGUE MENOS",
            "Netflix - Assinatura mensal"
        ]
        
        # Categorias esperadas
        categorias_esperadas = [
            "Transporte",
            "Streaming", 
            "Supermercado",
            "Alimenta√ß√£o",
            "Investimentos",
            "Investimentos",
            "Medicamentos",
            "Streaming"
        ]
        
        print("üö® TESTANDO CASOS QUE FALHARAM EM PRODU√á√ÉO:")
        acertos = 0
        
        for i, (caso, esperada) in enumerate(zip(casos_criticos, categorias_esperadas)):
            try:
                # Processar texto
                texto_processado = self.processador_texto.processar_texto(caso)
                
                # Vectorizar
                X_text = self.vectorizer.transform([texto_processado])
                
                # Features num√©ricas (valores fict√≠cios para demonstra√ß√£o)
                valor_abs = 50.0
                eh_receita = 1 if "Recebida" in caso or "Resgate" in caso else 0
                valor_log = np.log1p(valor_abs)
                tamanho_descricao = len(caso)
                tem_pix = 1 if "pix" in caso.lower() else 0
                tem_debito = 1 if "d√©bito" in caso.lower() else 0
                tem_transferencia = 1 if "transfer√™ncia" in caso.lower() else 0
                
                X_num = np.array([[valor_abs, eh_receita, valor_log, tamanho_descricao, 
                                 tem_pix, tem_debito, tem_transferencia]])
                X_num_scaled = self.scaler.transform(X_num)
                
                # Combinar features
                X_combined = hstack([X_text, X_num_scaled])
                
                # Predi√ß√£o
                predicao = self.modelo_final.predict(X_combined)[0]
                probabilidades = self.modelo_final.predict_proba(X_combined)[0]
                confianca = max(probabilidades)
                
                # Verificar acerto
                acertou = predicao == esperada
                if acertou:
                    acertos += 1
                
                status = "‚úÖ" if acertou else "‚ùå"
                print(f"{status} {caso[:50]:<50} ‚Üí {predicao:<15} (esperado: {esperada:<15}) [{confianca:.3f}]")
                
            except Exception as e:
                print(f"‚ùå Erro no caso {i+1}: {e}")
        
        taxa_acerto = (acertos / len(casos_criticos)) * 100
        print(f"\nüìä RESULTADO DOS CASOS CR√çTICOS:")
        print(f"   Acertos: {acertos}/{len(casos_criticos)} ({taxa_acerto:.1f}%)")
        
        if taxa_acerto >= 80:
            print(f"   üéâ EXCELENTE! Problemas de produ√ß√£o resolvidos!")
        elif taxa_acerto >= 60:
            print(f"   ‚úÖ BOM! Melhoria significativa esperada")
        else:
            print(f"   ‚ö†Ô∏è  ATEN√á√ÉO! Ainda h√° problemas a resolver")

def main():
    """Fun√ß√£o principal"""
    print("Iniciando treinamento com solu√ß√£o emergencial...")
    
    # Carregar dados
    df = carregar_dados()
    if df is None:
        return
    
    # Criar treinador
    treinador = TreinadorModeloCorrigido()
    
    # Pr√©-processar
    df_processado = treinador.preprocessar_dados(df)
    
    # Criar features
    X, y = treinador.criar_features(df_processado)
    
    # Treinar modelos
    X_test, y_test, y_pred = treinador.treinar_modelos(X, y)
    
    if X_test is not None:
        # Salvar modelo
        treinador.salvar_modelo()
        
        # Demonstra√ß√£o pr√°tica
        treinador.demonstracao_pratica(df_processado)
        
        print(f"\nüéâ TREINAMENTO CONCLU√çDO COM SUCESSO!")
        print(f"üéØ Modelo otimizado para resolver problemas de produ√ß√£o")
        print(f"üìÅ Arquivos salvos: modelo_final.pkl, vectorizer_final.pkl, etc.")
        print(f"\nüöÄ PR√ìXIMO PASSO:")
        print(f"   Execute: python classificar_arquivo_real.py")
        print(f"   Use seu CSV real para testar a melhoria!")
    else:
        print("‚ùå Falha no treinamento!")

if __name__ == "__main__":
    main()

